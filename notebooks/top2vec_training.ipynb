{
 "cells": [
  {
   "source": [
    "## Train a top2vec model\n",
    "\n",
    "### Part I: default model (total nr of topics are used)\n",
    "### Part II: hierarchically reduced model (100 topics)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndjson\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import top2vec\n",
    "from top2vec import Top2Vec "
   ]
  },
  {
   "source": [
    "### Load primitives and filter events longer than 50 characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/work/62138/corpus/primitives_220329/primitives_annotated.ndjson') as f:\n",
    "    data = ndjson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter events longer than 50 characters\n",
    "data_filtered = []\n",
    "for item in data:\n",
    "    if len(item['text']) > 50:\n",
    "        data_filtered.append(item)\n",
    "\n",
    "# prepare data for top2vec, create two lists of texts and document_ids\n",
    "corpus = []\n",
    "document_ids = []\n",
    "\n",
    "for key in data_filtered:\n",
    "    text = key['text']\n",
    "    id = key['id']\n",
    "    document_ids.append(id)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "source": [
    "## Part I: default model\n",
    "\n",
    "### Train, save and load a top2vec model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = Top2Vec(corpus, document_ids = document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('../models/top2vec/top2vecmodel_50_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = Top2Vec.load('/work/62138/models/top2vec/top2vecmodel_50_2')"
   ]
  },
  {
   "source": [
    "### Show 5 topics most close to keyword"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"aardbeving\"], num_topics=5)\n",
    "for topic in topic_nums:\n",
    "    model.generate_topic_wordcloud(topic)"
   ]
  },
  {
   "source": [
    "### Get topic distribution per document\n",
    "\n",
    "The build matrix is too large, so this has to be done in multiple runs. The output-files are saved, and merged afterwards"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very unpretty way to get the topics per document in one file, part one\n",
    "chunk_ids = document_ids[100000:113000]\n",
    "\n",
    "topic_ids, topic_vals, words, word_scores = model.get_documents_topics(doc_ids = chunk_ids, num_topics=415)\n",
    "\n",
    "\n",
    "representations = []\n",
    "for doc_topic_ids, doc_topic_vals in zip(topic_ids, topic_vals):\n",
    "    representations.append(\n",
    "        doc_topic_vals[doc_topic_ids]\n",
    "    )\n",
    "\n",
    "merged_rep = [list(l) for l in zip(chunk_ids, representations)]\n",
    "np.save('/work/62138/models/representations_5.npy', merged_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge files\n",
    "\n",
    "merged_rep_1 = np.load('/work/62138/models/representations_1.npy', allow_pickle=True).tolist()\n",
    "merged_rep_2 = np.load('/work/62138/models/representations_2.npy', allow_pickle=True).tolist()\n",
    "merged_rep_3 = np.load('/work/62138/models/representations_3.npy', allow_pickle=True).tolist()\n",
    "merged_rep_4 = np.load('/work/62138/models/representations_4.npy', allow_pickle=True).tolist()\n",
    "merged_rep_5 = np.load('/work/62138/models/representations_5.npy', allow_pickle=True).tolist()\n",
    "\n",
    "merged = merged_rep_1 + merged_rep_2 + merged_rep_3 + merged_rep_4 + merged_rep_5\n",
    "np.save('/work/62138/models/representation_final_50.npy', merged)"
   ]
  },
  {
   "source": [
    "## Part II: hierarchically reduced model (100 topics)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchically reduce model to 100 topics\n",
    "\n",
    "model.hierarchical_topic_reduction(100)"
   ]
  },
  {
   "source": [
    "### Get topic distribution per document\n",
    "\n",
    "The build matrix is too large, so this has to be done in multiple runs. The output-files are saved, and merged afterwards"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics per document distribution (reduced)\n",
    "chunk_ids = document_ids[60000:120000]\n",
    "\n",
    "topic_ids, topic_vals, words, word_scores = model.get_documents_topics(doc_ids = chunk_ids, reduced=True, num_topics=100)\n",
    "\n",
    "representations = []\n",
    "for doc_topic_ids, doc_topic_vals in zip(topic_ids, topic_vals):\n",
    "    representations.append(\n",
    "        doc_topic_vals[doc_topic_ids]\n",
    "    )\n",
    "\n",
    "merged_rep = [list(l) for l in zip(chunk_ids, representations)]\n",
    "np.save('/work/62138/models/representations_reduced_2.npy', merged_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge files\n",
    "\n",
    "merged_rep_1 = np.load('/work/62138/models/representations_reduced_1.npy', allow_pickle=True).tolist()\n",
    "merged_rep_2 = np.load('/work/62138/models/representations_reduced_2.npy', allow_pickle=True).tolist()\n",
    "\n",
    "\n",
    "merged = merged_rep_1 + merged_rep_2\n",
    "np.save('/work/62138/models/representation_final_50_reduced.npy', merged)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}